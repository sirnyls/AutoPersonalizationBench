{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os \n",
    "\n",
    "def perform_t_test_and_save(model_name, sae_path, output_csv_path):\n",
    "    # Load data from text files\n",
    "    df = pd.read_csv(sae_path, sep=\";\")\n",
    "    df['ukGT_usGT_score'] =  pd.to_numeric(df['ukGT_usGT_score'], errors='coerce')\n",
    "    df = df[df['ukGT_usGT_score'] < 0.9]\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert dataframes to numpy arrays\n",
    "    sample_us = df.us_score.to_numpy()\n",
    "    sample_gt = df.ukGT_usGT_score.to_numpy()\n",
    "    sample_uk = df.uk_score.to_numpy()\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_gt_uk = stats.ttest_ind(sample_uk, sample_gt, alternative='greater')\n",
    "    result_gt_us = stats.ttest_ind(sample_us, sample_gt, alternative='greater')\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, \"GroundTruth_UK\", result_gt_uk.statistic, result_gt_uk.pvalue],\n",
    "        [model_name, \"GroundTruth_US\", result_gt_us.statistic, result_gt_us.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Statistic\", \"P-Value\"])\n",
    "    \n",
    "    # Check if the CSV file already exists to decide on adding a header\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "    \n",
    "    # Save to CSV, append if file exists, include header if file does not exist\n",
    "    df.to_csv(output_csv_path, mode='a', index=False, header=not file_exists)\n",
    "    \n",
    "    print(f\"Results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n",
      "Results saved to t_test_results_case1.csv\n"
     ]
    }
   ],
   "source": [
    "perform_t_test_and_save(\n",
    "    \"Llama13B\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_llama13B.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "perform_t_test_and_save(\n",
    "    \"Llama7B\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_llama7B.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "perform_t_test_and_save(\n",
    "    \"Vicuna13B\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_vicuna13B.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "perform_t_test_and_save(\n",
    "    \"Vicuna7B\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_vicuna7B.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "perform_t_test_and_save(\n",
    "    \"Alpaca\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_alpaca.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "perform_t_test_and_save(\n",
    "    \"GPT-4\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/gpt_results.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "\n",
    "perform_t_test_and_save(\n",
    "    \"Gemini\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/gemini_results.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")\n",
    "\n",
    "perform_t_test_and_save(\n",
    "    \"Llama-70B\",\n",
    "    \"/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/llama_results.csv\",\n",
    "    \"t_test_results_case1.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('/Users/nils/AutoPersonalizationBench/case1_CulturalSensitivity/results/results_alpaca.csv', sep=\";\")\n",
    "df['ukGT_usGT_score'] =  pd.to_numeric(df['ukGT_usGT_score'], errors='coerce')\n",
    "df = df[df['ukGT_usGT_score'] < 0.9]\n",
    "\n",
    "# Convert dataframes to numpy arrays\n",
    "sample_us = df.us_score.to_numpy()\n",
    "sample_gt = df.ukGT_usGT_score.to_numpy()\n",
    "sample_uk = df.uk_score.to_numpy()\n",
    "\n",
    "# Perform KS tests\n",
    "result_gt_uk = stats.ttest_ind(sample_uk, sample_gt)\n",
    "result_gt_us = stats.ttest_ind(sample_us, sample_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True False True\n",
      "False False False\n"
     ]
    }
   ],
   "source": [
    "print(np.isnan(sample_us).any(), np.isnan(sample_gt).any(), np.isnan(sample_uk).any())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Unnamed: 0.2  Unnamed: 0.1  Unnamed: 0  \\\n",
      "3               3             3           3   \n",
      "182           182           182         182   \n",
      "656           656           656         656   \n",
      "764           764           764         764   \n",
      "\n",
      "                                              question  \\\n",
      "3    Do you think that using military force against...   \n",
      "182  Overall, are you satisfied or dissatisfied wit...   \n",
      "656  Please tell me for each of the following state...   \n",
      "764  Please tell me for each of the following thing...   \n",
      "\n",
      "                                            selections  \\\n",
      "3    {'United States': [0.22916666666666666, 0.4583...   \n",
      "182  {'United States': [0.32323232323232326, 0.6464...   \n",
      "656  {'United States': [0.424, 0.129, 0.098, 0.055,...   \n",
      "764  {'United States': [0.0, 0.022000000000000002, ...   \n",
      "\n",
      "                                               options  \\\n",
      "3    ['Often be justified', 'Sometimes be justified...   \n",
      "182        ['Satisfied', 'Dissatisfied', 'DK/Refused']   \n",
      "656  ['Never justifiable', '2', '3', '4', '5', '6',...   \n",
      "764  ['It is against democracy (spontaneous)', 'Not...   \n",
      "\n",
      "                                     options_formatted source  value_us  \\\n",
      "3    (A) Often be justified\\n(B) Sometimes be justi...    GAS  0,458334   \n",
      "182    (A) Satisfied\\n(B) Dissatisfied\\n(C) DK/Refused    GAS  0,646466   \n",
      "656  (A) Never justifiable\\n(B) 2\\n(C) 3\\n(D) 4\\n(E...    WVS  0,424001   \n",
      "764  (A) It is against democracy (spontaneous)\\n(B)...    WVS  0,373001   \n",
      "\n",
      "     value_uk  ...                                       options_dict  \\\n",
      "3    0,510205  ...  {'A': 'Often be justified', 'B': 'Sometimes be...   \n",
      "182  0,760001  ...  {'A': 'Satisfied', 'B': 'Dissatisfied', 'C': '...   \n",
      "656  0,497001  ...  {'A': 'Never justifiable', 'B': '2', 'C': '3',...   \n",
      "764  0,321001  ...  {'A': 'It is against democracy (spontaneous)',...   \n",
      "\n",
      "    model_answer_us  model_answer_uk model_answer_us_extract  \\\n",
      "3                 A              NaN                       A   \n",
      "182               A              NaN                       A   \n",
      "656               a                a                       a   \n",
      "764               s                K                       s   \n",
      "\n",
      "    model_answer_uk_extract  model_answer_us_option_match  \\\n",
      "3                       NaN            Often be justified   \n",
      "182                     NaN                     Satisfied   \n",
      "656                       a                           NaN   \n",
      "764                       K                           NaN   \n",
      "\n",
      "                 model_answer_uk_option_match  us_score uk_score  \\\n",
      "3                                         NaN  0.760871      NaN   \n",
      "182                                       NaN  0.690477      NaN   \n",
      "656                                       NaN       NaN      NaN   \n",
      "764  An essential characteristic of democracy       NaN      1.0   \n",
      "\n",
      "    ukGT_usGT_score  \n",
      "3          0.000000  \n",
      "182        0.747255  \n",
      "656        0.888891  \n",
      "764        0.750001  \n",
      "\n",
      "[4 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "nan_rows_specific = df[['us_score', 'ukGT_usGT_score', 'uk_score']].isna().any(axis=1)\n",
    "print(df[nan_rows_specific])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

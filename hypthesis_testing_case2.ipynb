{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os \n",
    "\n",
    "def perform_ks_tests_and_save(model_name, sae_path, esl_path, aae_path, output_csv_path):\n",
    "    # Load data from text files\n",
    "    SAE = pd.read_csv(sae_path, sep=\"\t\", header=None)\n",
    "    ESL = pd.read_csv(esl_path, sep=\"\t\", header=None)\n",
    "    AAE = pd.read_csv(aae_path, sep=\"\t\", header=None)\n",
    "    \n",
    "    # Convert dataframes to numpy arrays\n",
    "    sample_sae = SAE[1].to_numpy()\n",
    "    sample_esl = ESL[1].to_numpy()\n",
    "    sample_aae = AAE[1].to_numpy()\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_sae_esl = stats.ks_2samp(sample_sae, sample_esl)\n",
    "    result_sae_aae = stats.ks_2samp(sample_sae, sample_aae)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, \"AES_Score\", \"SAE-ESL\", result_sae_esl.statistic, result_sae_esl.pvalue],\n",
    "        [model_name, \"AES_Score\", \"SAE-AAE\", result_sae_aae.statistic, result_sae_aae.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "    \n",
    "    # Check if the CSV file already exists to decide on adding a header\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "    \n",
    "    # Save to CSV, append if file exists, include header if file does not exist\n",
    "    df.to_csv(output_csv_path, mode='a', index=False, header=not file_exists)\n",
    "    \n",
    "    print(f\"Results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ks_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "perform_ks_tests_and_save(\n",
    "    \"Alpaca\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_base_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_ESL_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_AAE_results.txt\",\n",
    "    \"ks_test_results_case2.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpaca = pd.read_csv('case2_EducationalDisparity/all_essays_alpaca.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'flesch_reading_ease': 61.67, 'smog_index': 11.5, 'flesch_kincaid_grade': 9.1, 'unique_words': 141, 'lexical_diversity': 0.4533762057877814, 'grammar_errors': 1}\""
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca.metrics_SAE[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import ast  # For converting string representation of a dictionary to a dictionary\n",
    "import os  # To check if the output CSV file already exists\n",
    "\n",
    "def perform_ks_tests_and_append_results(model_name, input_csv_path, metric, output_csv_path):\n",
    "    # Load data from the CSV file\n",
    "    data_df = pd.read_csv(input_csv_path, sep=';')\n",
    "    \n",
    "    # Convert string representations of dictionaries into actual dictionaries\n",
    "    data_df['metrics_SAE'] = data_df['metrics_SAE'].apply(ast.literal_eval)\n",
    "    data_df['metrics_AAE'] = data_df['metrics_AAE'].apply(ast.literal_eval)\n",
    "    data_df['metrics_ESL'] = data_df['metrics_ESL'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Extract the specified metric into numpy arrays\n",
    "    sample_sae = np.array([d[metric] for d in data_df['metrics_SAE']])\n",
    "    sample_aae = np.array([d[metric] for d in data_df['metrics_AAE']])\n",
    "    sample_esl = np.array([d[metric] for d in data_df['metrics_ESL']])\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_sae_esl = stats.ks_2samp(sample_sae, sample_esl)\n",
    "    result_sae_aae = stats.ks_2samp(sample_sae, sample_aae)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, metric, \"SAE-ESL\", result_sae_esl.statistic, result_sae_esl.pvalue],\n",
    "        [model_name, metric, \"SAE-AAE\", result_sae_aae.statistic, result_sae_aae.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Check if output CSV file exists; if so, append. Otherwise, create a new file.\n",
    "    if os.path.exists(output_csv_path):\n",
    "        df_existing = pd.read_csv(output_csv_path)\n",
    "        df_new = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "        df_updated = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_updated.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Results appended to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results appended to ks_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "perform_ks_tests_and_append_results(\n",
    "    \"Gemini\",\n",
    "    \"case2_EducationalDisparity/all_essays_gemini.csv\",\n",
    "    \"lexical_diversity\",\n",
    "    \"ks_test_results_case2.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'flesch_reading_ease'\n",
    "'lexical_diversity'\n",
    "'grammar_errors'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv('ks_test_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows where pvalue is less than 0.05: 31\n",
      "Total number of rows:  64\n"
     ]
    }
   ],
   "source": [
    "filtered_rows = results[results[\"P-Value\"] < 0.05]\n",
    "# Count the number of rows\n",
    "num_rows = len(filtered_rows)\n",
    "print(f\"Number of rows where pvalue is less than 0.05: {num_rows}\")\n",
    "print(f\"Total number of rows: \", len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import ast  # For converting string representation of a dictionary to a dictionary\n",
    "import os  # To check if the output CSV file already exists\n",
    "\n",
    "def perform_t_test_and_append_results(model_name, input_csv_path, metric, output_csv_path):\n",
    "    # Load data from the CSV file\n",
    "    data_df = pd.read_csv(input_csv_path, sep=';')\n",
    "    \n",
    "    # Convert string representations of dictionaries into actual dictionaries\n",
    "    data_df['metrics_SAE'] = data_df['metrics_SAE'].apply(ast.literal_eval)\n",
    "    data_df['metrics_AAE'] = data_df['metrics_AAE'].apply(ast.literal_eval)\n",
    "    data_df['metrics_ESL'] = data_df['metrics_ESL'].apply(ast.literal_eval)\n",
    "    \n",
    "    # Extract the specified metric into numpy arrays\n",
    "    sample_sae = np.array([d[metric] for d in data_df['metrics_SAE']])\n",
    "    sample_aae = np.array([d[metric] for d in data_df['metrics_AAE']])\n",
    "    sample_esl = np.array([d[metric] for d in data_df['metrics_ESL']])\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_sae_esl = stats.ttest_ind(sample_sae, sample_esl)\n",
    "    result_sae_aae = stats.ttest_ind(sample_sae, sample_aae)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, metric, \"SAE-ESL\", result_sae_esl.statistic, result_sae_esl.pvalue],\n",
    "        [model_name, metric, \"SAE-AAE\", result_sae_aae.statistic, result_sae_aae.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Check if output CSV file exists; if so, append. Otherwise, create a new file.\n",
    "    if os.path.exists(output_csv_path):\n",
    "        df_existing = pd.read_csv(output_csv_path)\n",
    "        df_new = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "        df_updated = pd.concat([df_existing, df_new], ignore_index=True)\n",
    "        df_updated.to_csv(output_csv_path, index=False)\n",
    "    else:\n",
    "        df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "        df.to_csv(output_csv_path, index=False)\n",
    "    \n",
    "    print(f\"Results appended to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results appended to t_test_results_case2.csv\n"
     ]
    }
   ],
   "source": [
    "perform_t_test_and_append_results(\n",
    "    \"Vicuna13B\",\n",
    "    \"case2_EducationalDisparity/all_essays_vicuna13B.csv\",\n",
    "    \"flesch_reading_ease\",\n",
    "    \"t_test_results_case2.csv\"\n",
    ")\n",
    "\n",
    "#'flesch_reading_ease'\n",
    "#'lexical_diversity'\n",
    "#'grammar_errors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os \n",
    "\n",
    "def perform_t_tests_and_save(model_name, sae_path, esl_path, aae_path, output_csv_path):\n",
    "    # Load data from text files\n",
    "    SAE = pd.read_csv(sae_path, sep=\"\t\", header=None)\n",
    "    ESL = pd.read_csv(esl_path, sep=\"\t\", header=None)\n",
    "    AAE = pd.read_csv(aae_path, sep=\"\t\", header=None)\n",
    "    \n",
    "    # Convert dataframes to numpy arrays\n",
    "    sample_sae = SAE[1].to_numpy()\n",
    "    sample_esl = ESL[1].to_numpy()\n",
    "    sample_aae = AAE[1].to_numpy()\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_sae_esl = stats.ttest_ind(sample_sae, sample_esl)\n",
    "    result_sae_aae = stats.ttest_ind(sample_sae, sample_aae)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, \"AES_Score\", \"SAE-ESL\", result_sae_esl.statistic, result_sae_esl.pvalue],\n",
    "        [model_name, \"AES_Score\", \"SAE-AAE\", result_sae_aae.statistic, result_sae_aae.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "    \n",
    "    # Check if the CSV file already exists to decide on adding a header\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "    \n",
    "    # Save to CSV, append if file exists, include header if file does not exist\n",
    "    df.to_csv(output_csv_path, mode='a', index=False, header=not file_exists)\n",
    "    \n",
    "    print(f\"Results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to t_test_results_case2.csv\n"
     ]
    }
   ],
   "source": [
    "perform_t_tests_and_save(\n",
    "    \"Vicuna7B\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/vicuna7B_base_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/vicuna7B_ESL_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/vicuna7B_AAE_results.txt\",\n",
    "    \"t_test_results_case2.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('t_test_two_sided_results_case2.csv')  # Replace 'your_file.csv' with your CSV file path\n",
    "\n",
    "\n",
    "threshold = 0.016666 # 0.05 / 3\n",
    "\n",
    "\n",
    "results = {'SAE-AAE': {}, 'SAE-ESL': {}}\n",
    "\n",
    "\n",
    "for pair in ['SAE-AAE', 'SAE-ESL']:\n",
    "\n",
    "    filtered_df = df[df['Pair'] == pair]\n",
    "    \n",
    "    # Group by model and check if any p-value is below the threshold for each model\n",
    "    for model, group in filtered_df.groupby('Model'):\n",
    "        significant = 'significant' if any(group['P-Value'] < threshold) else 'not significant'\n",
    "        results[pair][model] = significant\n",
    "\n",
    "# Save the results\n",
    "with open('t_test_significance_results_case2.txt', 'w') as file:\n",
    "    for pair, models in results.items():\n",
    "        file.write(f\"Results for {pair}:\\n\")\n",
    "        for model, result in models.items():\n",
    "            file.write(f\"{model}: {result}\\n\")\n",
    "        file.write(\"\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

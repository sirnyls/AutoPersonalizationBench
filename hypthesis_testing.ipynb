{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "SAE = pd.read_csv(\"case2_EducationalDisparity/results/MultiScaleBertAESResults/gpt_base_results.txt\", sep=\"\t\", header=None)\n",
    "ESL = pd.read_csv(\"case2_EducationalDisparity/results/MultiScaleBertAESResults/gpt_ESL_results.txt\", sep=\"\t\", header=None)\n",
    "AAE = pd.read_csv(\"case2_EducationalDisparity/results/MultiScaleBertAESResults/gpt_AAE_results.txt\", sep=\"\t\", header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sae = SAE[1].to_numpy()\n",
    "sample_ESL = ESL[1].to_numpy()\n",
    "sample_AAE = AAE[1].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KstestResult(statistic=0.5363636363636364, pvalue=2.3423551125504076e-14, statistic_location=0.008006727644226289, statistic_sign=-1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model metric pair statistic\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "rng = np.random.default_rng()\n",
    "sample1 = stats.uniform.rvs(size=100, random_state=rng)\n",
    "sample2 = stats.norm.rvs(size=110, random_state=rng)\n",
    "stats.ks_2samp(sample1, sample2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis: Quality of SAE is not the same as AAE/ESL. If p-value <0.05 -> reject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sae_esl = stats.ks_2samp(sample_sae, sample_ESL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_sae_aae = stats.ks_2samp(sample_sae, sample_AAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import os \n",
    "\n",
    "def perform_ks_tests_and_save(model_name, sae_path, esl_path, aae_path, output_csv_path):\n",
    "    # Load data from text files\n",
    "    SAE = pd.read_csv(sae_path, sep=\"\t\", header=None)\n",
    "    ESL = pd.read_csv(esl_path, sep=\"\t\", header=None)\n",
    "    AAE = pd.read_csv(aae_path, sep=\"\t\", header=None)\n",
    "    \n",
    "    # Convert dataframes to numpy arrays\n",
    "    sample_sae = SAE[1].to_numpy()\n",
    "    sample_esl = ESL[1].to_numpy()\n",
    "    sample_aae = AAE[1].to_numpy()\n",
    "    \n",
    "    # Perform KS tests\n",
    "    result_sae_esl = stats.ks_2samp(sample_sae, sample_esl)\n",
    "    result_sae_aae = stats.ks_2samp(sample_sae, sample_aae)\n",
    "    \n",
    "    # Prepare data for CSV\n",
    "    data = [\n",
    "        [model_name, \"AES_Score\", \"SAE-ESL\", result_sae_esl.statistic, result_sae_esl.pvalue],\n",
    "        [model_name, \"AES_Score\", \"SAE-AAE\", result_sae_aae.statistic, result_sae_aae.pvalue]\n",
    "    ]\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame(data, columns=[\"Model\", \"Metric\", \"Pair\", \"Statistic\", \"P-Value\"])\n",
    "    \n",
    "    # Check if the CSV file already exists to decide on adding a header\n",
    "    file_exists = os.path.isfile(output_csv_path)\n",
    "    \n",
    "    # Save to CSV, append if file exists, include header if file does not exist\n",
    "    df.to_csv(output_csv_path, mode='a', index=False, header=not file_exists)\n",
    "    \n",
    "    print(f\"Results saved to {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results saved to ks_test_results.csv\n"
     ]
    }
   ],
   "source": [
    "perform_ks_tests_and_save(\n",
    "    \"Alpaca\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_base_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_ESL_results.txt\",\n",
    "    \"case2_EducationalDisparity/results/MultiScaleBertAESResults/alpaca_AAE_results.txt\",\n",
    "    \"ks_test_results.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example for similarity calculation of llama results. \n",
    "## Same procedure for the other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "goqa = pd.read_csv('data_merged_GOQA_llama_70B.csv', sep=';')\n",
    "gpt = pd.read_csv('data_merged_gpt_llama_70B.csv', sep=';')\n",
    "quiz = pd.read_csv('data_merged_quiz_llama_70B.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
    ")\n",
    "\n",
    "def gpt4_text_similarity(text1, text2, model=\"gpt-4-1106-preview\"):\n",
    "    \"\"\"\n",
    "    Measures the similarity between two texts using GPT-4.\n",
    "\n",
    "    Returns:\n",
    "    float: A similarity score between 1 (not similar) and 5(very similar).\n",
    "    \"\"\"\n",
    "\n",
    "    system = f\"On a scale between 1 and 5, how similar are the following two sentences? Respond only with a score between 1 and 5.\"\n",
    "    prompt = f\"\"\"\n",
    "                Examples:\n",
    "                Input: \n",
    "                Sentence 1: Not really, too busy for that.\n",
    "                Sentence 2: Yes, quite fond of academic journals.\n",
    "                Output: 1.0\n",
    "                Input: \n",
    "                Sentence 1: Universal healthcare, accessible to all residents.\n",
    "                Sentence 2: NHS provides universal healthcare for all.\n",
    "                Output: 2.0\n",
    "                Input: \n",
    "                Sentence 1: Based on recommendations and personal interests.\n",
    "                Sentence 2: By author, genre, recommendations, and reviews.\n",
    "                Output: 3.0\n",
    "                Input: \n",
    "                Sentence 1: As often as I can.\n",
    "                Sentence 2: Quite often, I'm always willing.\n",
    "                Output: 4.0\n",
    "                Input: \n",
    "                Sentence 1: Yes, I love wearing hats!\n",
    "                Sentence 2: Yes, I quite fancy wearing hats.\n",
    "                Output: 5.0    \n",
    "                \\nInput:\\n\\nSentence 1: {text1}\\n\\n Sentence 2: {text2}. \\n Output:\"\"\"\n",
    "    #prompt = f\"Rate the similarity between the following two texts on a scale from 0 (completely different) to 1 (identical):\\n\\nText 1: {text1}\\n\\nText 2: {text2}\"\n",
    "    response = client.chat.completions.create(\n",
    "    model=model,\n",
    "    messages=[\n",
    "        {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": system\n",
    "        },\n",
    "        {\n",
    "        \"role\": \"user\",\n",
    "        \"content\":  prompt\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=20\n",
    "    )\n",
    "    #print(response.choices[0].message.content)\n",
    "    # Extracting the similarity score from the response\n",
    "    try:\n",
    "        last_message = response.choices[0].message.content\n",
    "        similarity_score = float(last_message.strip())\n",
    "    except (ValueError, KeyError, IndexError):\n",
    "        similarity_score = None\n",
    "    print(similarity_score)\n",
    "    return similarity_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_column(df, column_name, new_column_name):\n",
    "    \"\"\"\n",
    "    Normalize the values in a DataFrame column to the range 0-1 and save them in a new column.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column to normalize.\n",
    "    column_name (str): The name of the column to normalize.\n",
    "    new_column_name (str): The name of the new column for the normalized values.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the additional normalized column.\n",
    "    \"\"\"\n",
    "    # Copy the DataFrame to avoid modifying the original data\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    # Apply Min-Max normalization\n",
    "    df_normalized[new_column_name] = (df_normalized[column_name] - 1) / (5 - 1)\n",
    "    \n",
    "    return df_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_small_value(df, column_name):\n",
    "    \"\"\"\n",
    "    Add 0.00001 to each value in the specified column of a DataFrame \n",
    "    if the value is not 0.0 or 1.0.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame containing the column.\n",
    "    column_name (str): The name of the column to modify.\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: A DataFrame with the modified column.\n",
    "    \"\"\"\n",
    "    # Define the lambda function for the condition\n",
    "    add_value = lambda x: x + 0.000001 if x not in [0.0, 1.0] else x\n",
    "\n",
    "    # Apply the function to the specified column\n",
    "    df[column_name] = df[column_name].apply(add_value)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uk_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_uk'], row['answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['uk_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "print(\"uk_score Finished\")\n",
    "\n",
    "print(\"us_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['us_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"us_score finished\")\n",
    "\n",
    "print(\"ukGT_usGT_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['ukGT_usGT_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"ukGT_usGT_score finished\")\n",
    "\n",
    "print(\"ukGT_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['model_answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['ukGT_usMA_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "print(\"ukGT_usMA_score finished\")\n",
    "\n",
    "print(\"ukMA_usGT_score start\")\n",
    "\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['ukMA_usGT_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usGT_score finished\")\n",
    "\n",
    "print(\"ukMA_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "gpt['ukMA_usMA_score'] = gpt.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usMA_score finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt.to_csv('results_gpt.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"uk_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_uk'], row['answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['uk_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"uk_score Finished\")\n",
    "\n",
    "print(\"us_score Start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['us_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"us_score finished\")\n",
    "\n",
    "print(\"ukGT_usGT_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['ukGT_usGT_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "\n",
    "print(\"ukGT_usGT_score finished\")\n",
    "\n",
    "print(\"ukGT_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_uk'], row['model_answer_us'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['ukGT_usMA_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"ukGT_usMA_score finished\")\n",
    "\n",
    "print(\"ukMA_usGT_score start\")\n",
    "\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['ukMA_usGT_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usGT_score finished\")\n",
    "\n",
    "print(\"ukMA_usMA_score start\")\n",
    "def apply_similarity(row):\n",
    "    try:\n",
    "        return gpt4_text_similarity(row['model_answer_us'], row['model_answer_uk'])\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row: {e}\")\n",
    "        return None\n",
    "\n",
    "quiz['ukMA_usMA_score'] = quiz.apply(apply_similarity, axis=1)\n",
    "print(\"ukMA_usMA_score finished\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz.to_csv('results_quiz.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz = normalize_column(quiz, 'us_score', 'us_score')\n",
    "quiz = normalize_column(quiz, 'uk_score', 'uk_score')\n",
    "quiz = normalize_column(quiz, 'ukGT_usGT_score', 'ukGT_usGT_score')\n",
    "quiz = normalize_column(quiz, 'ukMA_usGT_score', 'ukMA_usGT_score')\n",
    "quiz = normalize_column(quiz, 'ukGT_usMA_score', 'ukGT_usMA_score')\n",
    "quiz = normalize_column(quiz, 'ukMA_usMA_score', 'ukMA_usMA_score')\n",
    "quiz = add_small_value(quiz, 'us_score')\n",
    "quiz = add_small_value(quiz, 'uk_score')\n",
    "quiz = add_small_value(quiz, 'ukGT_usGT_score')\n",
    "quiz = add_small_value(quiz, 'ukMA_usGT_score')\n",
    "quiz = add_small_value(quiz, 'ukGT_usMA_score')\n",
    "quiz = add_small_value(quiz, 'ukMA_usMA_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = normalize_column(gpt, 'us_score', 'us_score')\n",
    "gpt = normalize_column(gpt, 'uk_score', 'uk_score')\n",
    "gpt = normalize_column(gpt, 'ukGT_usGT_score', 'ukGT_usGT_score')\n",
    "gpt = normalize_column(gpt, 'ukMA_usGT_score', 'ukMA_usGT_score')\n",
    "gpt = normalize_column(gpt, 'ukGT_usMA_score', 'ukGT_usMA_score')\n",
    "gpt = normalize_column(gpt, 'ukMA_usMA_score', 'ukMA_usMA_score')\n",
    "gpt = add_small_value(gpt, 'us_score')\n",
    "gpt = add_small_value(gpt, 'uk_score')\n",
    "gpt = add_small_value(gpt, 'ukGT_usGT_score')\n",
    "gpt = add_small_value(gpt, 'ukMA_usGT_score')\n",
    "gpt = add_small_value(gpt, 'ukGT_usMA_score')\n",
    "gpt = add_small_value(gpt, 'ukMA_usMA_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "def extract_letter(s):\n",
    "    match = re.search(r'\\((.*?)\\)', s)\n",
    "    return match.group(1) if match else None\n",
    "\n",
    "# Apply the function to the DataFrame column\n",
    "goqa['model_answer_us'] = goqa['model_answer_us'].apply(extract_letter)\n",
    "goqa['model_answer_uk'] = goqa['model_answer_uk'].apply(extract_letter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast \n",
    "def extract_value_from_dict(df, dict_col, key_col, value_col):\n",
    "    # Convert string representations of dictionaries to actual dictionaries if needed\n",
    "    df[dict_col] = df[dict_col].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
    "\n",
    "    # Function to get value from the dictionary based on the key\n",
    "    def get_value(row):\n",
    "        dict_data = row[dict_col]\n",
    "        key = row[key_col]\n",
    "        # Check if dict_data is a dictionary and key is not None\n",
    "        if isinstance(dict_data, dict) and key is not None:\n",
    "            return dict_data.get(key, None)  # Returns None if the key is not found\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    # Apply the function to each row and store the result in a new column\n",
    "    df[value_col] = df.apply(get_value, axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assume df is your DataFrame, 'dict_column' is the name of the column with dictionaries,\n",
    "# 'key_column' is the name of the column with keys, and you want to store values in 'value_column'\n",
    "goqa = extract_value_from_dict(goqa, 'options_dict', 'model_answer_us', 'model_answer_us_option_match')\n",
    "goqa = extract_value_from_dict(goqa, 'options_dict', 'model_answer_uk', 'model_answer_uk_option_match')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa['model_answer_us_option_match'] = goqa['model_answer_us_option_match'].astype(str)\n",
    "goqa['model_answer_uk_option_match'] = goqa['model_answer_uk_option_match'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Score for questions with answer options\n",
    "def calculate_score_gt(row):\n",
    "    if row['question type'] in ['Likert Scale', 'Numerical Scale', 'Ordinal Scale'] and row['#_options'] > 2:\n",
    " \n",
    "        ground_truth = row['model_answer_uk_option_match']\n",
    "        model_answer = row['answer_us']\n",
    "        options = row['options']\n",
    "        #print(type(ground_truth))\n",
    "        #print(type(model_answer))\n",
    "        # Normalize the positions of the answers in the options list to a 0-1 range\n",
    "        gt_index = options.index(ground_truth) / (len(options) - 1)\n",
    "        model_index = options.index(model_answer) / (len(options) - 1)\n",
    "\n",
    "        # Calculate the absolute error\n",
    "        error = abs(gt_index - model_index)\n",
    "\n",
    "        # Score can be inversely related to the error (1 - error)\n",
    "        score = 1 - error\n",
    "        return score\n",
    "    else: \n",
    "        return int(row['model_answer_uk_option_match'] == row['answer_us'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa['ukMA_usGT_score'] = goqa.apply(calculate_score_gt, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa = add_small_value(goqa, 'us_score')\n",
    "goqa = add_small_value(goqa, 'uk_score')\n",
    "goqa = add_small_value(goqa, 'ukGT_usGT_score')\n",
    "goqa = add_small_value(goqa, 'ukMA_usGT_score')\n",
    "goqa = add_small_value(goqa, 'ukGT_usMA_score')\n",
    "goqa = add_small_value(goqa, 'ukMA_usMA_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "goqa.to_csv('results_goqa.csv', sep=';')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
